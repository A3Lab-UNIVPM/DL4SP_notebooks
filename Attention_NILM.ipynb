{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Attention_NILM.ipynb",
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## ATTENTION-NILM\n",
        "\n",
        "This notebook reproduces the work published by Piccialli et al. 2020 for load disaggregation.\n",
        "The notebook structure is the following:\n",
        "\n",
        "\n",
        "1.   Libraries import and class definition\n",
        "2.   Network definition\n",
        "3.   Metrics definition\n",
        "4.   Data loading and pre-processing\n",
        "5.   Network training\n",
        "6.   Network inference and test scores\n",
        "\n"
      ],
      "metadata": {
        "id": "YTeze-zC9wTe"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UV0Hkdxu5Mk9"
      },
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from matplotlib import pyplot as plt\n",
        "from tensorflow.python.keras.utils.data_utils import Sequence\n",
        "import tensorflow.keras as keras\n",
        "import pandas as pd\n",
        "import os\n",
        "\n",
        "tf.random.set_seed(7)"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Definition of DataGenerator class\n",
        "\n",
        "This class derives from keras Sequence used to iterate over the data and create signal windows disjointed or overlapped.\n",
        "\n",
        "The class creates the batch containing the windows of aggregate and both target sequences (one for classification and one for disaggregation) to be passed to the network.\n",
        "\n",
        "Number of windows considered in the batch depends on the batch size, defined subsequently."
      ],
      "metadata": {
        "id": "FzNkOycJAh4A"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_CDhqooW56c0"
      },
      "source": [
        "class DataGenerator(tf.keras.utils.Sequence):\n",
        "\n",
        "    def __init__(self, mains, appliances_regression, appliances_classification, window_size, batch_size, shuffle=False):\n",
        "        self.mains = mains\n",
        "        self.appliances_regression = appliances_regression\n",
        "        self.appliances_classification = appliances_classification\n",
        "        self.window_size = window_size\n",
        "        self.batch_size = batch_size\n",
        "        self.indices = np.arange(len(self.mains) - self.window_size + 1)\n",
        "        self.shuffle = shuffle\n",
        "\n",
        "    def __len__(self):\n",
        "        return int(np.ceil(len(self.indices) / self.batch_size))\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        mains_batch = []\n",
        "        appliances_regression_batch = []\n",
        "        appliances_classification_batch = []\n",
        "        appliance_regression_sample = []\n",
        "        appliance_classification_sample = []\n",
        "\n",
        "        if idx == self.__len__() - 1:\n",
        "            inds = self.indices[idx * self.batch_size:]\n",
        "        else:\n",
        "            inds = self.indices[idx * self.batch_size: (idx + 1) * self.batch_size]\n",
        "\n",
        "        for i in inds:\n",
        "            main_sample = self.mains[i:i + self.window_size]\n",
        "            appliance_regression_sample = self.appliances_regression[i:i + self.window_size]\n",
        "            appliance_classification_sample = self.appliances_classification[i:i + self.window_size]\n",
        "\n",
        "            mains_batch.append(main_sample)\n",
        "            appliances_regression_batch.append(appliance_regression_sample)\n",
        "            appliances_classification_batch.append(appliance_classification_sample)\n",
        "\n",
        "        mains_batch_np = np.array(mains_batch)\n",
        "        mains_batch_np = np.reshape(mains_batch_np, (mains_batch_np.shape[0], mains_batch_np.shape[1], 1))\n",
        "        appliances_regression_batch_np = np.array(appliances_regression_batch)\n",
        "        appliances_regression_batch_np = np.reshape(appliances_regression_batch_np,\n",
        "                                                    (appliances_regression_batch_np.shape[0],\n",
        "                                                     appliances_regression_batch_np.shape[1]))\n",
        "        appliances_classification_batch_np = np.array(appliances_classification_batch)\n",
        "        appliances_classification_batch_np = np.reshape(appliances_classification_batch_np,\n",
        "                                                        (appliances_classification_batch_np.shape[0],\n",
        "                                                         appliances_classification_batch_np.shape[1]))\n",
        "\n",
        "        return mains_batch_np, (appliances_regression_batch_np, appliances_classification_batch_np)\n",
        "\n",
        "    def on_epoch_end(self):\n",
        "\n",
        "        if self.shuffle:\n",
        "            np.random.shuffle(self.indices)\n",
        "\n"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Definition of network structure and AttentionLayer class\n",
        "\n",
        "Definition of the attention layer as a keras model and network creation.\n",
        "\n",
        "The network is composed of one regression branch and one classification branch. Outputs are multiplied and the loss is computed on the final output.\n"
      ],
      "metadata": {
        "id": "SB2uKgYDCHpx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class AttentionLayer(tf.keras.Model):\n",
        "\n",
        "    def __init__(self, units):\n",
        "        super(AttentionLayer, self).__init__()\n",
        "        weight_initializer = 'he_normal'\n",
        "        self.W = tf.keras.layers.Dense(units, kernel_initializer=weight_initializer)\n",
        "        self.V = tf.keras.layers.Dense(1, kernel_initializer=weight_initializer)\n",
        "\n",
        "    def call(self, encoder_output, **kwargs):\n",
        "        # encoder_output shape == (batch_size, seq_length, latent_dim)\n",
        "        # score shape == (batch_size, seq_length, 1)\n",
        "        # we get 1 at the last axis because we are applying score to self.V\n",
        "        # the shape of the tensor before applying self.V is (batch_size, seq_length, units)\n",
        "        score = self.V(tf.nn.tanh(self.W(encoder_output)))\n",
        "\n",
        "        # attention_weights shape == (batch_size, seq_length, 1)\n",
        "        attention_weights = tf.nn.softmax(score, axis=1)\n",
        "\n",
        "        # context_vector shape after sum == (batch_size, hidden_size)\n",
        "        context_vector = attention_weights * encoder_output\n",
        "        context_vector = tf.reduce_sum(context_vector, axis=1)\n",
        "        return context_vector, attention_weights\n",
        "\n",
        "def build_model(window_size, filters, kernel_size, units):\n",
        "    input_data = tf.keras.Input(shape=(window_size, 1))\n",
        "\n",
        "    # CLASSIFICATION SUBNETWORK\n",
        "    x = tf.keras.layers.Conv1D(filters=30, kernel_size=10, activation='relu')(input_data)\n",
        "    x = tf.keras.layers.Conv1D(filters=30, kernel_size=8, activation='relu')(x)\n",
        "    x = tf.keras.layers.Conv1D(filters=40, kernel_size=6, activation='relu')(x)\n",
        "    x = tf.keras.layers.Conv1D(filters=50, kernel_size=5, activation='relu')(x)\n",
        "    x = tf.keras.layers.Conv1D(filters=50, kernel_size=5, activation='relu')(x)\n",
        "    x = tf.keras.layers.Conv1D(filters=50, kernel_size=5, activation='relu')(x)\n",
        "    x = tf.keras.layers.Flatten()(x)\n",
        "    x = tf.keras.layers.Dense(units=1024, activation='relu', kernel_initializer='he_normal')(x)\n",
        "    classification_output = tf.keras.layers.Dense(units=window_size, activation='sigmoid', name=\"classification_output\")(x)\n",
        "\n",
        "    #REGRESSION SUBNETWORK\n",
        "    y = tf.keras.layers.Conv1D(filters=filters, kernel_size=kernel_size, activation='relu')(input_data)\n",
        "    y = tf.keras.layers.Conv1D(filters=filters, kernel_size=kernel_size, activation='relu')(y)\n",
        "    y = tf.keras.layers.Conv1D(filters=filters, kernel_size=kernel_size, activation='relu')(y)\n",
        "    y = tf.keras.layers.Conv1D(filters=filters, kernel_size=kernel_size, activation='relu')(y)\n",
        "    y = tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(units, activation=\"tanh\", return_sequences=True), merge_mode=\"concat\")(y)\n",
        "    y, weights = AttentionLayer(units=units)(y)\n",
        "    y = tf.keras.layers.Dense(units, activation='relu')(y)\n",
        "    regression_output = tf.keras.layers.Dense(window_size, activation='relu', name=\"regression_output\")(y)\n",
        "\n",
        "    output = tf.keras.layers.Multiply(name=\"output\")([regression_output, classification_output])\n",
        "\n",
        "    full_model = tf.keras.Model(inputs=input_data, outputs=[output, classification_output], name=\"LDwA\")\n",
        "    attention_model = tf.keras.Model(inputs=input_data, outputs=weights)\n",
        "\n",
        "    optimizer = tf.keras.optimizers.SGD(learning_rate=0.01, momentum=0.9)\n",
        "\n",
        "    full_model.compile(optimizer=optimizer, loss={\n",
        "        \"output\": tf.keras.losses.MeanSquaredError(),\n",
        "        \"classification_output\": tf.keras.losses.BinaryCrossentropy()}, metrics= [\"mae\",\"mae\"])\n",
        "\n",
        "\n",
        "    return full_model, attention_model"
      ],
      "metadata": {
        "id": "cfjrGTEXBYqA"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This function is defined to reconstruct the original sequence after the windowing."
      ],
      "metadata": {
        "id": "Z_DQzq8ZD9Gn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def build_overall_sequence(sequences):\n",
        "    unique_sequence = []\n",
        "    matrix = [sequences[::-1, :].diagonal(i) for i in range(-sequences.shape[0] + 1, sequences.shape[1])]\n",
        "    for i in range(len(matrix)):\n",
        "        unique_sequence.append(np.median(matrix[i]))\n",
        "    unique_sequence = np.array(unique_sequence)\n",
        "    return unique_sequence\n"
      ],
      "metadata": {
        "id": "Z0DFXa9mBb0f"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Metrics\n",
        "\n",
        "Definition of metrics for performance evaluation.\n"
      ],
      "metadata": {
        "id": "unDUOgt6EBBu"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ps7S5qsK5olO"
      },
      "source": [
        "# METRICS FOR PERFORMANCE EVALUATION\n",
        "def mae(prediction, true):\n",
        "    MAE = abs(true - prediction)\n",
        "    MAE = np.sum(MAE)\n",
        "    MAE = MAE / len(prediction)\n",
        "    return MAE\n",
        "\n",
        "\n",
        "def sae(prediction, true, N):\n",
        "    T = len(prediction)\n",
        "    K = int(T / N)\n",
        "    SAE = 0\n",
        "    for k in range(1, N):\n",
        "        pred_r = np.sum(prediction[k * N: (k + 1) * N])\n",
        "        true_r = np.sum(true[k * N: (k + 1) * N])\n",
        "        SAE += abs(true_r - pred_r)\n",
        "    SAE = SAE / (K * N)\n",
        "    return SAE\n",
        "\n",
        "\n",
        "def f1(prediction, true):\n",
        "    epsilon = 1e-8\n",
        "    TP = epsilon\n",
        "    FN = epsilon\n",
        "    FP = epsilon\n",
        "    TN = epsilon\n",
        "    for i in range(len(prediction)):\n",
        "        if prediction[i] >= 0.5:\n",
        "            prediction_binary = 1\n",
        "        else:\n",
        "            prediction_binary = 0\n",
        "        if prediction_binary == 1 and true[i] == 1:\n",
        "            TP += 1\n",
        "        elif prediction_binary == 0 and true[i] == 1:\n",
        "            FN += 1\n",
        "        elif prediction_binary == 1 and true[i] == 0:\n",
        "            FP += 1\n",
        "        elif prediction_binary == 0 and true[i] == 0:\n",
        "            TN += 1\n",
        "    R = TP / (TP + FN)\n",
        "    P = TP / (TP + FP)\n",
        "    f1 = (2 * P * R) / (P + R)\n",
        "    return f1"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Data pre-processing functions definition"
      ],
      "metadata": {
        "id": "wzeTUKHvEjM_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def standardize_data(data, mu=0.0, sigma=1.0):\n",
        "    data -= mu\n",
        "    data /= sigma\n",
        "    return data\n",
        "\n",
        "\n",
        "def normalize_data(data, min_value=0.0, max_value=1.0):\n",
        "    data -= min_value\n",
        "    data /= max_value - min_value\n",
        "    return data"
      ],
      "metadata": {
        "id": "Q8bjf7jzEgwk"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Data Loading and pre-processing\n",
        "\n",
        "Training, validation and test sets are loaded and pre-processed.\n",
        "\n",
        "The aggregate signals are standardize while the regression output is normalized. The classification target vector is binarized to be feed into the network with a threshold of 15 Watt for each appliance."
      ],
      "metadata": {
        "id": "SBAgSym_Ey_S"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# download data\n",
        "!wget -nc -O NILM_data.zip \"https://drive.google.com/uc?export=download&id=18pwM_ie7M3DgLyZ5zSSEURneWoiq2IF2\"\n",
        "!unzip -q NILM_data.zip"
      ],
      "metadata": {
        "id": "ioidmvI1X0VF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mjGBnuZ5wD3n"
      },
      "source": [
        "# Read the NILM dataset\n",
        "\n",
        "appliance = 'kettle'\n",
        "plot = True\n",
        "\n",
        "val_data = np.array(pd.read_csv(os.path.join(appliance, appliance + '_validation_.csv')))\n",
        "train_data = np.array(pd.read_csv(os.path.join(appliance, appliance + '_training_.csv')))\n",
        "test_data = np.array(pd.read_csv(os.path.join(appliance, appliance + '_test_.csv'), skiprows=1500000))\n",
        "\n",
        "main_train, appliance_train = train_data[:,0], train_data[:,1]\n",
        "main_val, appliance_val = val_data[:,0], val_data[:,1]\n",
        "main_test, appliance_test = test_data[:,0], test_data[:,1]\n",
        "\n",
        "if plot:\n",
        "\n",
        "        plt.plot(main_train)\n",
        "        plt.plot(appliance_train)\n",
        "        plt.legend(['Aggregate', appliance], loc='upper left')\n",
        "        plt.title(\"Train set\")\n",
        "        plt.show()\n",
        "        plt.plot(main_val)\n",
        "        plt.plot(appliance_val)\n",
        "        plt.legend(['Aggregate', appliance], loc='upper left')\n",
        "        plt.title(\"Validation set\")\n",
        "        plt.show()\n",
        "        plt.plot(main_test)\n",
        "        plt.plot(appliance_test)\n",
        "        plt.legend(['Aggregate', appliance], loc='upper left')\n",
        "        plt.title(\"Test set\")\n",
        "        plt.show()\n",
        "\n",
        "# Threshold of 15 Watt for detecting the ON/OFF states\n",
        "\n",
        "THRESHOLD = 15\n",
        "appliance_train_classification = np.copy(appliance_train)\n",
        "appliance_train_classification[appliance_train_classification <= THRESHOLD] = 0\n",
        "appliance_train_classification[appliance_train_classification > THRESHOLD] = 1\n",
        "\n",
        "appliance_val_classification = np.copy(appliance_val)\n",
        "appliance_val_classification[appliance_val_classification <= THRESHOLD] = 0\n",
        "appliance_val_classification[appliance_val_classification > THRESHOLD] = 1\n",
        "\n",
        "# Standardization of the main power and normalization of appliance power\n",
        "appliance_min_power = np.min(appliance_train)\n",
        "appliance_max_power = np.max(appliance_train)\n",
        "main_std = np.std(main_train)\n",
        "main_mean = np.mean(main_train)\n",
        "\n",
        "main_train = standardize_data(main_train, np.mean(main_train), np.std(main_train))\n",
        "main_val = standardize_data(main_val, np.mean(main_val), np.std(main_val))\n",
        "\n",
        "appliance_train_regression = np.copy(appliance_train)\n",
        "appliance_train_regression = normalize_data(appliance_train_regression, appliance_min_power, appliance_max_power)\n",
        "\n",
        "appliance_val_regression = np.copy(appliance_val)\n",
        "appliance_val_regression = normalize_data(appliance_val_regression, appliance_min_power, appliance_max_power)\n",
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Network Training\n",
        "\n",
        "Firstly, the window dimension and the hyperparameters have to be selected based on the desired appliance.\n",
        "\n",
        "Then, the model was built and trained monitoring the validation loss.  "
      ],
      "metadata": {
        "id": "T-OLlWMyGMuz"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IF0yJ2n3087K"
      },
      "source": [
        "# Choose the appliance-specific window size and hyperparms\n",
        "window_size = 0\n",
        "\n",
        "if appliance == 'kettle':\n",
        "        window_size = 128\n",
        "        filters = 32\n",
        "        kernel_size = 4\n",
        "        units = 128\n",
        "if appliance == 'microwave':\n",
        "        window_size = 256\n",
        "        filters = 32\n",
        "        kernel_size = 4\n",
        "        units = 256\n",
        "if appliance == 'fridge':\n",
        "        window_size = 512\n",
        "        filters = 32\n",
        "        kernel_size = 8\n",
        "        units = 256\n",
        "if appliance == 'dishwasher':\n",
        "        window_size = 1024\n",
        "        filters = 64\n",
        "        kernel_size = 16\n",
        "        units = 256\n",
        "if appliance == 'washingmachine':\n",
        "        window_size = 1536\n",
        "        filters = 16\n",
        "        kernel_size = 4\n",
        "        units = 512\n",
        "\n",
        "\n",
        "# Dataset generator\n",
        "batch_size = 32\n",
        "train_generator = DataGenerator(main_train, appliance_train_regression,\n",
        "                                    appliance_train_classification, window_size, batch_size)\n",
        "val_generator = DataGenerator(main_val, appliance_val_regression,\n",
        "                                  appliance_val_classification, window_size, batch_size)\n",
        "\n",
        "train_steps = train_generator.__len__()\n",
        "validation_steps = val_generator.__len__()\n",
        "\n",
        "model, att_model = build_model(window_size, filters, kernel_size, units)\n",
        "model.summary()\n",
        "\n",
        "early_stop = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)\n",
        "\n",
        "history = model.fit(x=train_generator, epochs=1, steps_per_epoch=train_steps,   #100\n",
        "                        validation_data=val_generator, validation_steps=validation_steps,\n",
        "                       callbacks=[early_stop], verbose=1)\n",
        "\n",
        "# Plotting the results of training\n",
        "\n",
        "history_dict = history.history\n",
        "plt.title('Loss during training')\n",
        "plt.plot(np.arange(len(history.epoch)), history_dict['loss'])\n",
        "plt.plot(np.arange(len(history.epoch)), history_dict['val_loss'])\n",
        "plt.legend(['train', 'val'])\n",
        "plt.show()\n",
        "plt.savefig(appliance + 'loss.png')\n",
        "\n",
        "model.save(appliance + '_weights.keras')\n",
        "model.save_weights(appliance + '.weights.h5')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Network Inference and scores\n",
        "\n",
        "Test data are pre-processed to be feed into newtork for inference. Both prediction and target are denormalized to compute the scores."
      ],
      "metadata": {
        "id": "zNNurZUBF-mg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Test\n",
        "appliance_test_classification = np.copy(appliance_test)\n",
        "appliance_test_classification[appliance_test_classification <= THRESHOLD] = 0\n",
        "appliance_test_classification[appliance_test_classification > THRESHOLD] = 1\n",
        "\n",
        "appliance_min_power = np.min(appliance_train)\n",
        "appliance_max_power = np.max(appliance_train)\n",
        "\n",
        "main_test = standardize_data(main_test, np.mean(main_test), np.std(main_test))\n",
        "\n",
        "appliance_test_regression = np.copy(appliance_test)\n",
        "appliance_test_regression = normalize_data(appliance_test_regression, appliance_min_power, appliance_max_power)\n",
        "\n",
        "batch_size = 32\n",
        "\n",
        "test_generator = DataGenerator(main_test, appliance_test_regression,\n",
        "                                    appliance_test_classification, window_size, batch_size)\n",
        "\n",
        "test_steps = test_generator.__len__()\n",
        "\n",
        "\n",
        "predicted_output, predicted_on_off = model.predict(x=test_generator, steps=test_steps)\n",
        "predicted_output *= (appliance_max_power - appliance_min_power)\n",
        "predicted_output += appliance_min_power\n",
        " # Clip negative values to zero\n",
        "predicted_output[predicted_output < 0] = 0.0\n",
        "\n",
        "prediction = build_overall_sequence(predicted_output)\n",
        "prediction_on_off = build_overall_sequence(predicted_on_off)\n",
        "\n",
        "# Compute metrics\n",
        "N = 1200\n",
        "MAE = mae(prediction, appliance_test)\n",
        "SAE_std = abs(np.sum(appliance_test, dtype=np.int64) - np.sum(prediction, dtype=np.int64)) / np.sum(appliance_test, dtype=np.int64)\n",
        "\n",
        "SAE = sae(prediction, appliance_test, N=N)\n",
        "F1 = f1(prediction_on_off, appliance_test_classification)\n",
        "\n",
        "print(\"MAE = {}\".format(MAE))\n",
        "print(\"SAE = {}\".format(SAE))\n",
        "print(\"SAE_std = {}\".format(SAE_std))\n",
        "print(\"F1 = {}\".format(F1))\n",
        "\n",
        "# Plot the result of the prediction\n",
        "fig, axes = plt.subplots(nrows=6, ncols=1, figsize=(50, 40))\n",
        "axes[0].set_title(\"Real\")\n",
        "axes[0].plot(np.arange(len(appliance_test)), appliance_test, color='blue')\n",
        "axes[1].set_title(\"Prediction\")\n",
        "axes[1].plot(np.arange(len(prediction)), prediction, color='orange')\n",
        "axes[2].set_title(\"Real vs prediction\")\n",
        "axes[2].plot(np.arange(len(appliance_test)), appliance_test, color='blue')\n",
        "axes[2].plot(np.arange(len(prediction)), prediction, color='orange')\n",
        "axes[3].set_title(\"Real on off\")\n",
        "axes[3].plot(np.arange(len(appliance_test_classification)), appliance_test_classification, color='blue')\n",
        "axes[4].set_title(\"Prediction on off\")\n",
        "axes[4].plot(np.arange(len(prediction_on_off)), prediction_on_off, color='orange')\n",
        "axes[5].set_title(\"Real vs Prediction on off\")\n",
        "axes[5].plot(np.arange(len(appliance_test_classification)), appliance_test_classification, color='blue')\n",
        "axes[5].plot(np.arange(len(prediction_on_off)), prediction_on_off, color='orange')\n",
        "fig.tight_layout()\n",
        "plt.show()\n",
        "plt.savefig(appliance + '.png')"
      ],
      "metadata": {
        "id": "uQwwDWoZF80q"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}